{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Avg Train Loss: 0.0207507331\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import itertools\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import HypergraphConv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "################################################\n",
    "# Hypergraph Data Generation and FD Manipulation\n",
    "################################################\n",
    "\n",
    "def generate_acyclic_hypergraph(num_attributes,\n",
    "                                            num_fds,\n",
    "                                            max_edge_size,\n",
    "                                            max_rhs_size):\n",
    "    \"\"\"\n",
    "    Generate a random acyclic set of FDs by:\n",
    "      1) Creating a random topological order of attributes.\n",
    "      2) Iteratively sampling (X -> Y) and adding edges if it does not create a cycle.\n",
    "    \"\"\"\n",
    "    attributes = [f\"A{i}\" for i in range(num_attributes)]\n",
    "    random.shuffle(attributes)  # random topological ordering\n",
    "\n",
    "    # Keep a directed graph to ensure no cycles are introduced\n",
    "    dependency_graph = nx.DiGraph()\n",
    "    for attr in attributes:\n",
    "        dependency_graph.add_node(attr)\n",
    "\n",
    "    hypergraph = {}  # dict: { tuple_of_LHS : set_of_RHS_attributes }\n",
    "    attempts = 0\n",
    "\n",
    "    while len(hypergraph) < num_fds and attempts < 10_000:\n",
    "        attempts += 1\n",
    "\n",
    "        # Randomly pick how many attributes in a potential FD\n",
    "        edge_size = random.randint(2, max_edge_size)\n",
    "        subset = random.sample(attributes, edge_size)\n",
    "\n",
    "        # Decide how many go on LHS vs. RHS (at least 1 each).\n",
    "        lhs_size = random.randint(1, min(edge_size - 1, max_edge_size - 1))\n",
    "        rhs_size = edge_size - lhs_size\n",
    "        if rhs_size > max_rhs_size:\n",
    "            continue\n",
    "\n",
    "        X = tuple(sorted(subset[:lhs_size]))\n",
    "        Y = set(subset[lhs_size:])\n",
    "        if not X or not Y:\n",
    "            continue\n",
    "\n",
    "        # Tentatively add edges (x -> y) for x in X, y in Y\n",
    "        edges_to_add = []\n",
    "        for x in X:\n",
    "            for y in Y:\n",
    "                edges_to_add.append((x, y))\n",
    "                dependency_graph.add_edge(x, y)\n",
    "\n",
    "        # Check acyclicity\n",
    "        if nx.is_directed_acyclic_graph(dependency_graph):\n",
    "            # Valid FD\n",
    "            if X not in hypergraph:\n",
    "                hypergraph[X] = set()\n",
    "            hypergraph[X].update(Y)\n",
    "        else:\n",
    "            # Revert edges\n",
    "            for (x, y) in edges_to_add:\n",
    "                if dependency_graph.has_edge(x, y):\n",
    "                    dependency_graph.remove_edge(x, y)\n",
    "\n",
    "    return hypergraph\n",
    "\n",
    "def compute_closure(fd_set, attributes):\n",
    "    \"\"\"\n",
    "    Given a dictionary of FDs: {X -> set_of_Y}, compute the closure of 'attributes'.\n",
    "    \"\"\"\n",
    "    closure = set(attributes)\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        for lhs, rhs_set in fd_set.items():\n",
    "            if set(lhs).issubset(closure) and not rhs_set.issubset(closure):\n",
    "                closure.update(rhs_set)\n",
    "                changed = True\n",
    "    return closure\n",
    "\n",
    "def decompose_fds_to_singleton(fd_set):\n",
    "    \"\"\"\n",
    "    Decompose multi-attribute RHS into singleton FDs: X -> A, X -> B, etc.\n",
    "    \"\"\"\n",
    "    new_fd_set = {}\n",
    "    for lhs, rhs_set in fd_set.items():\n",
    "        for rhs in rhs_set:\n",
    "            lhs_sorted = tuple(sorted(lhs))\n",
    "            if lhs_sorted not in new_fd_set:\n",
    "                new_fd_set[lhs_sorted] = set()\n",
    "            new_fd_set[lhs_sorted].add(rhs)\n",
    "    return new_fd_set\n",
    "\n",
    "def reduce_fd_left_side(fd_set):\n",
    "    \"\"\"\n",
    "    Attempt to remove redundant attributes from LHS of each FD\n",
    "    (standard minimal cover step).\n",
    "    \"\"\"\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        all_fds = list(fd_set.items())\n",
    "        for lhs, rhs_set in all_fds:\n",
    "            lhs_set = set(lhs)\n",
    "            for attr in list(lhs_set):\n",
    "                lhs_reduced = lhs_set - {attr}\n",
    "                if not lhs_reduced:\n",
    "                    continue\n",
    "\n",
    "                original_rhs = fd_set.get(tuple(sorted(lhs_set)), set())\n",
    "                if tuple(sorted(lhs_set)) in fd_set:\n",
    "                    del fd_set[tuple(sorted(lhs_set))]\n",
    "\n",
    "                lhs_red_tuple = tuple(sorted(lhs_reduced))\n",
    "                if lhs_red_tuple not in fd_set:\n",
    "                    fd_set[lhs_red_tuple] = set()\n",
    "                fd_set[lhs_red_tuple].update(rhs_set)\n",
    "\n",
    "                # Check closure\n",
    "                test_closure = compute_closure(fd_set, lhs_reduced)\n",
    "                if not rhs_set.issubset(test_closure):\n",
    "                    # revert\n",
    "                    del fd_set[lhs_red_tuple]\n",
    "                    fd_set[tuple(sorted(lhs_set))] = original_rhs\n",
    "                else:\n",
    "                    changed = True\n",
    "                    break\n",
    "    return fd_set\n",
    "\n",
    "def remove_redundant_fds(fd_set):\n",
    "    \"\"\"\n",
    "    Remove entire FDs if they are implied by the others (still standard minimal cover).\n",
    "    \"\"\"\n",
    "    fds_list = list(fd_set.items())\n",
    "    for lhs, rhs_set in fds_list:\n",
    "        for rhs in list(rhs_set):\n",
    "            old_rhs = set(fd_set.get(lhs, []))\n",
    "            old_rhs.remove(rhs)\n",
    "            if not old_rhs:\n",
    "                del fd_set[lhs]\n",
    "            else:\n",
    "                fd_set[lhs] = old_rhs\n",
    "\n",
    "            c = compute_closure(fd_set, lhs)\n",
    "            if rhs not in c:\n",
    "                if lhs not in fd_set:\n",
    "                    fd_set[lhs] = set()\n",
    "                fd_set[lhs].add(rhs)\n",
    "    return fd_set\n",
    "\n",
    "def minimal_cover(fd_set):\n",
    "    \"\"\"\n",
    "    1) Decompose multi-attribute RHS -> singletons\n",
    "    2) Reduce LHS\n",
    "    3) Remove redundant FDs\n",
    "    \"\"\"\n",
    "    fd_set = decompose_fds_to_singleton(fd_set)\n",
    "    fd_set = reduce_fd_left_side(fd_set)\n",
    "    fd_set = remove_redundant_fds(fd_set)\n",
    "    return fd_set\n",
    "\n",
    "def is_cover_minimal(fd_set):\n",
    "    \"\"\"\n",
    "    Check if the FD set is a valid minimal cover.\n",
    "    \"\"\"\n",
    "    return len(fd_set) == len(minimal_cover(fd_set))\n",
    "\n",
    "def is_cyclic_fd_graph(fd_set):\n",
    "    \"\"\"\n",
    "    Check if a functional dependency set forms a cyclic graph.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for X, Y_set in fd_set.items():\n",
    "        for y in Y_set:\n",
    "            for x in X:\n",
    "                G.add_edge(x, y)\n",
    "\n",
    "    return not nx.is_directed_acyclic_graph(G) \n",
    "\n",
    "def has_same_closure(fd_set1, fd_set2, attributes):\n",
    "    \"\"\"\n",
    "    Check if two FD sets have the same closure.\n",
    "    \"\"\"\n",
    "    closure1 = compute_closure(fd_set1, attributes)\n",
    "    closure2 = compute_closure(fd_set2, attributes)\n",
    "    return closure1 == closure2\n",
    "\n",
    "def compute_3NF_decomposition(fd_set, attributes):\n",
    "    \"\"\"\n",
    "    Compute a 3NF decomposition from a set of functional dependencies.\n",
    "    \"\"\"\n",
    "    relations = []\n",
    "\n",
    "    # Create relations for each FD\n",
    "    for X, Y_set in fd_set.items():\n",
    "        new_relation = set(X) | Y_set\n",
    "        relations.append(new_relation)\n",
    "\n",
    "    # Ensure lossless join by adding a candidate key relation\n",
    "    candidate_key = find_candidate_key(fd_set, attributes)\n",
    "\n",
    "    if not isinstance(candidate_key, set):  \n",
    "        candidate_key = set(candidate_key)\n",
    "\n",
    "    if not any(candidate_key.issubset(rel) for rel in relations):\n",
    "        relations.append(candidate_key)\n",
    "\n",
    "    return relations\n",
    "\n",
    "def find_candidate_key(fd_set, attributes):\n",
    "    \"\"\"\n",
    "    Find a candidate key for a given relation based on functional dependencies.\n",
    "    \n",
    "    :param fd_set: The minimal cover of functional dependencies.\n",
    "    :param attributes: The full set of attributes in the relation.\n",
    "    :return: A candidate key as a set of attributes.\n",
    "    \"\"\"\n",
    "    closure_sets = {}\n",
    "\n",
    "    for X in fd_set.keys():\n",
    "        closure = compute_closure(fd_set, X)\n",
    "        closure_sets[frozenset(X)] = closure\n",
    "\n",
    "    for X, closure in closure_sets.items():\n",
    "        if closure == set(attributes):\n",
    "            return set(X)  # Ensure this always returns a set\n",
    "\n",
    "    return {str(a) for a in attributes}  # Default\n",
    "\n",
    "################################################\n",
    "# FASTR Implementation\n",
    "################################################\n",
    "\n",
    "def compute_fastr_closure(fd_set, attributes):\n",
    "    \"\"\"\n",
    "    Compute closure using partition-based refinement.\n",
    "    \"\"\"\n",
    "    closure = set(attributes)\n",
    "    changed = True\n",
    "\n",
    "    while changed:\n",
    "        changed = False\n",
    "        for X, Y_set in fd_set.items():\n",
    "            if set(X).issubset(closure) and not Y_set.issubset(closure):\n",
    "                closure.update(Y_set)\n",
    "                changed = True\n",
    "    \n",
    "    return closure\n",
    "\n",
    "def refine_partitions(partitions, X, Y):\n",
    "    \"\"\"\n",
    "    Merge partitions if X → Y holds.\n",
    "    \"\"\"\n",
    "    merged_partition = set()\n",
    "    # Union of all partitions containing X\n",
    "    for attr in X:\n",
    "        merged_partition |= partitions[attr]  \n",
    "\n",
    "    # Merge Y into the refined partition\n",
    "    for y in Y:\n",
    "        partitions[y] |= merged_partition  \n",
    "\n",
    "def fastr_minimal_cover(fd_set):\n",
    "    \"\"\"\n",
    "    Compute minimal cover by removing redundant FDs.\n",
    "    \"\"\"\n",
    "    min_cover = defaultdict(set)\n",
    "    \n",
    "    for X, Y_set in fd_set.items():\n",
    "        for y in Y_set:\n",
    "            temp_FD_set = copy.deepcopy(fd_set)  # Deep copy to avoid modifying original\n",
    "            if X in temp_FD_set:\n",
    "                temp_FD_set[X] -= {y}  # Remove y from X's dependency\n",
    "\n",
    "            # If y is still derivable from X, it's redundant and should be removed\n",
    "            if y not in compute_fastr_closure(temp_FD_set, X):\n",
    "                min_cover[X].add(y)  # Retain only necessary FDs\n",
    "                \n",
    "    return min_cover\n",
    "\n",
    "def run_FASTR(hypergraph):\n",
    "    \"\"\"\n",
    "    Full FASTR algorithm: partition refinement + minimal cover computation.\n",
    "    \"\"\"\n",
    "    fastr_fds = defaultdict(set)\n",
    "    attributes = set(attr for X, Y_set in hypergraph.items() for attr in X + tuple(Y_set))\n",
    "\n",
    "    # Initialize partitions: Each attribute starts in its own set\n",
    "    partitions = {attr: {attr} for attr in attributes}\n",
    "\n",
    "    for X, Y_set in hypergraph.items():\n",
    "        X_tuple = tuple(sorted(X))\n",
    "        closure = compute_fastr_closure(hypergraph, X)\n",
    "        \n",
    "        # Prune redundant attributes\n",
    "        minimal_closure = {y for y in closure if y not in X_tuple}\n",
    "        \n",
    "        # Partition refinement step\n",
    "        refine_partitions(partitions, X_tuple, minimal_closure)\n",
    "        \n",
    "        # Store FDs\n",
    "        fastr_fds[X_tuple] |= minimal_closure\n",
    "    \n",
    "    # Compute minimal cover\n",
    "    return fastr_minimal_cover(fastr_fds)\n",
    "\n",
    "################################################\n",
    "# Create Dataset with Labels\n",
    "################################################\n",
    "\n",
    "def encode_hypergraph_torch(hypergraph, num_attributes):\n",
    "    \"\"\"\n",
    "    Convert a full hypergraph into PyTorch tensor format.\n",
    "    \"\"\"\n",
    "    edge_index = []\n",
    "    attribute_to_index = {attr: idx for idx, attr in enumerate(set(itertools.chain(*[X + tuple(Y_set) for X, Y_set in hypergraph.items()])))}\n",
    "\n",
    "    for hyperedge_id, (X, Y_set) in enumerate(hypergraph.items()):\n",
    "        for attr in X:\n",
    "            edge_index.append([attribute_to_index[attr], hyperedge_id])\n",
    "        for attr in Y_set:\n",
    "            edge_index.append([attribute_to_index[attr], hyperedge_id])\n",
    "\n",
    "    return torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "################################################\n",
    "# HGNN Model and Loss Functions\n",
    "################################################\n",
    "\n",
    "class HNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(HNN, self).__init__()\n",
    "        self.conv1 = HypergraphConv(in_channels, hidden_channels)\n",
    "        self.conv2 = HypergraphConv(hidden_channels, out_channels)\n",
    "        self.loss_history = []\n",
    "        self.validation_loss_history = []\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "def compute_join_cost_loss(decomposed_relations, original_table_size):\n",
    "    \"\"\"\n",
    "    Compute the join cost loss for a given 3NF decomposition.\n",
    "    \"\"\"\n",
    "    join_loss = 0\n",
    "    for i in range(len(decomposed_relations)):\n",
    "        for j in range(i + 1, len(decomposed_relations)):\n",
    "            common_attrs = decomposed_relations[i].intersection(decomposed_relations[j])\n",
    "            if common_attrs:\n",
    "                selectivity = min(0.1, 1.0 / len(common_attrs))  # Approximate selectivity\n",
    "                surviving_tuples = original_table_size * selectivity\n",
    "                join_loss += surviving_tuples * (len(decomposed_relations[i]) * len(decomposed_relations[j]))\n",
    "    \n",
    "    return join_loss / original_table_size  # Normalize by total tuples\n",
    "\n",
    "def left_reduction_loss(fd_set):\n",
    "    \"\"\"\n",
    "    Computes the left-reduction loss by penalizing FDs where an attribute in the LHS can be removed while maintaining closure.\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    for X, Y_set in list(fd_set.items()):\n",
    "        X = list(X)\n",
    "        for attr in X[:]:\n",
    "            X_reduced = tuple(sorted(set(X) - {attr}))\n",
    "            if X_reduced and X_reduced in fd_set and compute_closure(fd_set, X_reduced) >= Y_set:\n",
    "                loss += 1  # Penalize redundant LHS attributes\n",
    "    return loss\n",
    "\n",
    "def right_reduction_loss(fd_set):\n",
    "    \"\"\"\n",
    "    Computes the right-reduction loss by ensuring that every FD has a single attribute on the RHS.\n",
    "    \"\"\"\n",
    "    return sum(len(Y_set) - 1 for Y_set in fd_set.values() if len(Y_set) > 1)\n",
    "\n",
    "def minimal_cover_consistency_loss(predicted_FDs, original_FDs, attributes):\n",
    "    \"\"\"\n",
    "    Ensures that the predicted FD cover maintains the same closure as the original FDs.\n",
    "    \"\"\"\n",
    "    predicted_closure = compute_closure(predicted_FDs, attributes)\n",
    "    original_closure = compute_closure(original_FDs, attributes)\n",
    "    return len(original_closure - predicted_closure) + len(predicted_closure - original_closure)  # Penalize closure mismatch\n",
    "\n",
    "def recall_boost_loss(predicted_FDs, ground_truth_FDs, attributes):\n",
    "    predicted_closure = compute_closure(predicted_FDs, attributes)\n",
    "    ground_truth_closure = compute_closure(ground_truth_FDs, attributes)\n",
    "    return max(len(ground_truth_closure - predicted_closure), 0)  # Penalize missing FDs\n",
    "\n",
    "def total_loss(predicted_FDs, original_FDs, decomposed_relations, original_table_size, lambda_join=1):\n",
    "    \"\"\"\n",
    "    Compute total loss for HNN optimization, incorporating redundancy penalties and minimality constraints.\n",
    "    \"\"\"\n",
    "    left_loss = left_reduction_loss(predicted_FDs)\n",
    "    right_loss = right_reduction_loss(predicted_FDs)\n",
    "    minimality_loss = minimal_cover_consistency_loss(predicted_FDs, original_FDs, list(original_FDs.keys()))\n",
    "    recall_loss = recall_boost_loss(predicted_FDs, original_FDs, list(original_FDs.keys()))\n",
    "\n",
    "    join_loss = compute_join_cost_loss(decomposed_relations, original_table_size)\n",
    "\n",
    "    \n",
    "    loss = left_loss + right_loss + minimality_loss + lambda_join * join_loss + recall_loss\n",
    "    print(loss)\n",
    "    return torch.tensor(loss, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "def train_hnn(model, train_hypergraphs, optimizer, criterion, num_attributes, epochs=100):\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for graph in train_hypergraphs:\n",
    "            edge_index = encode_hypergraph_torch(graph, num_attributes)\n",
    "            train_data = Data(x=torch.eye(num_attributes), edge_index=edge_index, y=torch.ones((num_attributes, 1)))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_data.x, train_data.edge_index)\n",
    "            loss = criterion(output, train_data.y)  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        model.loss_history.append(epoch_loss / len(train_hypergraphs))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Avg Train Loss: {epoch_loss / len(train_hypergraphs):.10f}\")\n",
    "\n",
    "################################################\n",
    "# Evaluation\n",
    "################################################\n",
    "\n",
    "def closure_equivalent(fd_set_pred, fd_set_true, attributes):\n",
    "    \"\"\"\n",
    "    Check if for all subsets of attributes (or at least the key ones),\n",
    "    the closure is the same. In practice, enumerating all subsets is expensive,\n",
    "    so we often do approximate checks or key-based checks.\n",
    "    Here, we do a simpler check: pick each LHS from fd_set_true,\n",
    "    see if closure matches. This can be adapted.\n",
    "    \"\"\"\n",
    "    # Minimally, let's check closure of each LHS in the true set.\n",
    "    for lhs, _rhs in fd_set_true.items():\n",
    "        if not has_same_closure(fd_set_pred, fd_set_true, lhs):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def count_redundant_dependencies(fd_set):\n",
    "    \"\"\"\n",
    "    Count the total number of redundant dependencies in the FD set.\n",
    "    \"\"\"\n",
    "    redundant_count = 0\n",
    "\n",
    "    for X, Y_set in list(fd_set.items()):\n",
    "        X = list(X)\n",
    "\n",
    "        # Left-redundancy: Check if an attribute can be removed\n",
    "        for attr in X[:]:\n",
    "            X_reduced = tuple(sorted(set(X) - {attr}))\n",
    "            if X_reduced and X_reduced in fd_set and compute_closure(fd_set, X_reduced) >= Y_set:\n",
    "                redundant_count += 1\n",
    "\n",
    "        # Right-redundancy: Check if multiple attributes in RHS\n",
    "        if len(Y_set) > 1:\n",
    "            redundant_count += len(Y_set) - 1  # Each extra attribute in RHS is redundant\n",
    "\n",
    "    return redundant_count\n",
    "\n",
    "def breakdown_redundancy_per_FD(fd_set):\n",
    "    \"\"\"\n",
    "    Returns a breakdown of left vs. right redundancy per FD.\n",
    "    \"\"\"\n",
    "    redundancy_report = {}\n",
    "\n",
    "    for X, Y_set in list(fd_set.items()):\n",
    "        X = list(X)\n",
    "        left_redundancy = []\n",
    "        right_redundancy = []\n",
    "\n",
    "        # Left-redundancy: Detect removable attributes\n",
    "        for attr in X[:]:\n",
    "            X_reduced = tuple(sorted(set(X) - {attr}))\n",
    "            if X_reduced and X_reduced in fd_set and compute_closure(fd_set, X_reduced) >= Y_set:\n",
    "                left_redundancy.append(attr)\n",
    "\n",
    "        # Right-redundancy: Detect extra RHS attributes\n",
    "        if len(Y_set) > 1:\n",
    "            right_redundancy.extend(list(Y_set)[1:])  # All except the first are redundant\n",
    "\n",
    "        redundancy_report[X] = {\n",
    "            \"left_redundant\": left_redundancy,\n",
    "            \"right_redundant\": right_redundancy\n",
    "        }\n",
    "\n",
    "    return redundancy_report\n",
    "\n",
    "def hamming_loss(predicted_FD_sets, ground_truth_FD_sets, attributes):\n",
    "    \"\"\"\n",
    "    Compute Hamming Loss across multiple hypergraphs.\n",
    "    The loss is the number of missing or extra FDs in the prediction.\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    num_samples = len(predicted_FD_sets)\n",
    "\n",
    "    for predicted_FDs, ground_truth_FDs in zip(predicted_FD_sets, ground_truth_FD_sets):\n",
    "        if len(predicted_FDs) <= len(ground_truth_FDs) and closure_equivalent(predicted_FDs, ground_truth_FDs, attributes) and is_cover_minimal(predicted_FDs):\n",
    "            pass \n",
    "\n",
    "        predicted_set = set((tuple(sorted(X)), tuple(sorted(Y))) for X, Y in predicted_FDs.items())\n",
    "        ground_truth_set = set((tuple(sorted(X)), tuple(sorted(Y))) for X, Y in ground_truth_FDs.items())\n",
    "\n",
    "        false_positives = len(predicted_set - ground_truth_set)  # Extra dependencies\n",
    "        false_negatives = len(ground_truth_set - predicted_set)  # Missing dependencies\n",
    "        loss = false_positives + false_negatives\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "    return total_loss / num_samples if num_samples > 0 else 0.0\n",
    "\n",
    "def compute_join_cost(decomposed_relations, original_table_size):\n",
    "    \"\"\"\n",
    "    Compute the join cost for a given 3NF decomposition.\n",
    "    \n",
    "    :param decomposed_relations: List of table sizes in the 3NF schema.\n",
    "    :param original_table_size: Size of the original unnormalized relation.\n",
    "    :return: Join cost loss.\n",
    "    \"\"\"\n",
    "    join_cost = 0\n",
    "    for i in range(len(decomposed_relations)):\n",
    "        for j in range(i + 1, len(decomposed_relations)):\n",
    "            join_cost += (len(decomposed_relations[i]) * len(decomposed_relations[j])) / original_table_size\n",
    "    return join_cost\n",
    "\n",
    "def evaluate_HNN(model, test_hypergraphs, ground_truth_FD_sets, num_attributes):\n",
    "    \"\"\"\n",
    "    Evaluate HNN model, apply FASTR post-processing to its predictions, and compare results.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    if not test_hypergraphs or not isinstance(test_hypergraphs[0], dict):\n",
    "        raise ValueError(\"Error: test_hypergraphs is empty or not properly formatted as dictionaries.\")\n",
    "\n",
    "    attributes = extract_attributes(test_hypergraphs)\n",
    "\n",
    "    hnn_time = 0\n",
    "    hnn_avg_len_change = 0\n",
    "    predicted_FD_sets = []\n",
    "\n",
    "    # HNN Evaluation\n",
    "    for test_hypergraph in test_hypergraphs:\n",
    "        start_len = len(test_hypergraph)\n",
    "        start_hnn = time.time()\n",
    "        with torch.no_grad():\n",
    "            test_edge_index = encode_hypergraph_torch(test_hypergraph, num_attributes)\n",
    "            test_data = Data(x=torch.eye(num_attributes), edge_index=test_edge_index, y=torch.ones((num_attributes, 1)))\n",
    "            test_output = model(test_data.x, test_data.edge_index)\n",
    "        end_hnn = time.time()\n",
    "\n",
    "        hnn_time += (end_hnn - start_hnn)\n",
    "        \n",
    "        predicted_FDs = {}  # New predicted FD set\n",
    "        threshold = 0.5  # Use a threshold for binarization\n",
    "\n",
    "        for idx, score in enumerate(test_output):\n",
    "            if score.item() > threshold:  # Only keep significant dependencies\n",
    "                lhs_idx = test_edge_index[0, idx].item()\n",
    "                rhs_idx = test_edge_index[1, idx].item()\n",
    "                lhs_attr = attributes[lhs_idx]\n",
    "                rhs_attr = attributes[rhs_idx]\n",
    "\n",
    "                if (lhs_attr,) not in predicted_FDs:\n",
    "                    predicted_FDs[(lhs_attr,)] = set()\n",
    "                predicted_FDs[(lhs_attr,)].add(rhs_attr)\n",
    "\n",
    "                \n",
    "        predicted_FD_sets.append(predicted_FDs)\n",
    "        hnn_avg_len_change += start_len - len(predicted_FDs)\n",
    "\n",
    "    \n",
    "    hnn_avg_len_change /= len(test_hypergraphs)\n",
    "    hnn_time /= len(test_hypergraphs)\n",
    "\n",
    "    # FASTR Evaluation on Original Test Set\n",
    "    fastr_time = 0\n",
    "    fastr_avg_len_change = 0\n",
    "    fastr_FD_sets = []\n",
    "\n",
    "    for test_hypergraph in test_hypergraphs:\n",
    "        start_len = len(test_hypergraph)\n",
    "        start_fastr = time.time()\n",
    "        fastr_FDs = run_FASTR(test_hypergraph)\n",
    "        end_fastr = time.time()\n",
    "\n",
    "        fastr_time += (end_fastr - start_fastr)\n",
    "        fastr_FD_sets.append(fastr_FDs)\n",
    "\n",
    "        fastr_avg_len_change += start_len - len(fastr_FDs)\n",
    "\n",
    "    fastr_avg_len_change /= len(test_hypergraphs)\n",
    "    fastr_time /= len(test_hypergraphs)\n",
    "\n",
    "    # Post-processing: Apply FASTR to HNN Predictions\n",
    "    hnn_fastr_time = 0\n",
    "    hnn_fastr_FD_sets = []\n",
    "    hnn_fastr_avg_len_change = 0\n",
    "\n",
    "    for hnn_pred in predicted_FD_sets:\n",
    "        start_len = len(hnn_pred)\n",
    "        start_hnn_fastr = time.time()\n",
    "        hnn_fastr_FDs = run_FASTR(hnn_pred)\n",
    "        end_hnn_fastr = time.time()\n",
    "\n",
    "        hnn_fastr_time += (end_hnn_fastr - start_hnn_fastr)\n",
    "        hnn_fastr_FD_sets.append(hnn_fastr_FDs)\n",
    "\n",
    "        hnn_fastr_avg_len_change += len(hnn_pred) - len(hnn_fastr_FDs)\n",
    "\n",
    "    hnn_fastr_avg_len_change /= len(predicted_FD_sets)\n",
    "    hnn_fastr_time /= len(predicted_FD_sets)\n",
    "\n",
    "    # Compute Metrics for HNN\n",
    "    hamming_hnn = hamming_loss(predicted_FD_sets, ground_truth_FD_sets, attributes)\n",
    "\n",
    "    # Compute Redundancy for HNN\n",
    "    total_redundant_HNN = sum(count_redundant_dependencies(fd_set) for fd_set in predicted_FD_sets)\n",
    "    left_redundancy_HNN = sum(left_reduction_loss(fd_set) for fd_set in predicted_FD_sets)\n",
    "    right_redundancy_HNN = sum(right_reduction_loss(fd_set) for fd_set in predicted_FD_sets)\n",
    "    not_min_covers_HNN = sum(not is_cover_minimal(fd_set) for fd_set in predicted_FD_sets)\n",
    "    not_same_closure_HNN = sum(\n",
    "        not closure_equivalent(pred_fd, gt_fd, attributes)\n",
    "        for pred_fd, gt_fd in zip(predicted_FD_sets, ground_truth_FD_sets)\n",
    "    )\n",
    "\n",
    "\n",
    "    # Compute Join Cost for HNN\n",
    "    join_cost_hnn = sum(\n",
    "        compute_join_cost(compute_3NF_decomposition(fd_set, attributes), len(fd_set))\n",
    "        for fd_set in predicted_FD_sets\n",
    "    ) / len(predicted_FD_sets)\n",
    "\n",
    "    # Compute Metrics for FASTR\n",
    "    hamming_fastr = hamming_loss(fastr_FD_sets, ground_truth_FD_sets, attributes)\n",
    "\n",
    "    # Compute Redundancy for FASTR\n",
    "    total_redundant_FASTR = sum(count_redundant_dependencies(fd_set) for fd_set in fastr_FD_sets)\n",
    "    left_redundancy_FASTR = sum(left_reduction_loss(fd_set) for fd_set in fastr_FD_sets)\n",
    "    right_redundancy_FASTR = sum(right_reduction_loss(fd_set) for fd_set in fastr_FD_sets)\n",
    "    not_min_covers_FASTR = sum(not is_cover_minimal(fd_set) for fd_set in fastr_FD_sets)\n",
    "    not_same_closure_FASTR = sum(\n",
    "        not closure_equivalent(fastr_fd, gt_fd, attributes)\n",
    "        for fastr_fd, gt_fd in zip(fastr_FD_sets, ground_truth_FD_sets)\n",
    "    )\n",
    "\n",
    "\n",
    "    # Compute Join Cost for FASTR\n",
    "    join_cost_fastr = sum(\n",
    "        compute_join_cost(compute_3NF_decomposition(fd_set, attributes), len(fd_set))\n",
    "        for fd_set in fastr_FD_sets\n",
    "    ) / len(fastr_FD_sets)\n",
    "\n",
    "    # Compute Metrics for HNN + FASTR Post-processing\n",
    "    hamming_hnn_fastr = hamming_loss(hnn_fastr_FD_sets, ground_truth_FD_sets, attributes)\n",
    "\n",
    "    # Compute Redundancy for HNN + FASTR\n",
    "    total_redundant_HNN_FASTR = sum(count_redundant_dependencies(fd_set) for fd_set in hnn_fastr_FD_sets)\n",
    "    left_redundancy_HNN_FASTR = sum(left_reduction_loss(fd_set) for fd_set in hnn_fastr_FD_sets)\n",
    "    right_redundancy_HNN_FASTR = sum(right_reduction_loss(fd_set) for fd_set in hnn_fastr_FD_sets)\n",
    "    not_min_covers_HNN_FASTR = sum(not is_cover_minimal(fd_set) for fd_set in hnn_fastr_FD_sets)\n",
    "    not_same_closure_HNN_FASTR = sum(\n",
    "        not closure_equivalent(hnn_fastr_fd, gt_fd, attributes)\n",
    "        for hnn_fastr_fd, gt_fd in zip(hnn_fastr_FD_sets, ground_truth_FD_sets)\n",
    "    )\n",
    "\n",
    "    # Compute Join Cost for HNN + FASTR\n",
    "    join_cost_hnn_fastr = sum(\n",
    "        compute_join_cost(compute_3NF_decomposition(fd_set, attributes), len(fd_set))\n",
    "        for fd_set in hnn_fastr_FD_sets\n",
    "    ) / len(hnn_fastr_FD_sets)\n",
    "\n",
    "    print(\"\\n🔍 **Evaluation Summary** 🔍\")\n",
    "    print(f\"📌 HNN - Hamming Loss: {hamming_hnn:.4f}, Join Cost: {join_cost_hnn:.2f}\")\n",
    "    print(f\"📌 FASTR - Hamming Loss: {hamming_fastr:.4f}, Join Cost: {join_cost_fastr:.2f}\")\n",
    "    print(f\"📌 HNN + FASTR - Hamming Loss: {hamming_hnn_fastr:.4f}, Join Cost: {join_cost_hnn_fastr:.2f}\")    \n",
    "\n",
    "    print(\"\\n📊 **Redundancy Analysis** 📊\")\n",
    "    print(f\"📌 HNN - Avg Length Change in covers: {hnn_avg_len_change}\")\n",
    "    print(f\"📌 HNN - Not Minimal Covers: {not_min_covers_HNN}\")\n",
    "    print(f\"📌 HNN - Not Same Closure: {not_same_closure_HNN}\")\n",
    "    print(f\"📌 HNN - Total Redundant Dependencies: {total_redundant_HNN}\")\n",
    "    print(f\"📌 HNN - Left Redundancy Loss: {left_redundancy_HNN}\")\n",
    "    print(f\"📌 HNN - Right Redundancy Loss: {right_redundancy_HNN}\")\n",
    "    print(f\"📌 FASTR - Avg Length Change in covers: {fastr_avg_len_change}\")\n",
    "    print(f\"📌 FASTR - Not Minimal Covers: {not_min_covers_FASTR}\")\n",
    "    print(f\"📌 FASTR - Not Same Closure: {not_same_closure_FASTR}\")\n",
    "    print(f\"📌 FASTR - Total Redundant Dependencies: {total_redundant_FASTR}\")\n",
    "    print(f\"📌 FASTR - Left Redundancy Loss: {left_redundancy_FASTR}\")\n",
    "    print(f\"📌 FASTR - Right Redundancy Loss: {right_redundancy_FASTR}\")\n",
    "    print(f\"📌 HNN + FASTR - Avg Additional Length Change in covers: {hnn_fastr_avg_len_change}\")\n",
    "    print(f\"📌 HNN + FASTR - Not Minimal Covers: {not_min_covers_HNN_FASTR}\")\n",
    "    print(f\"📌 HNN + FASTR - Not Same Closure: {not_same_closure_HNN_FASTR}\")\n",
    "    print(f\"📌 HNN + FASTR - Total Redundant Dependencies: {total_redundant_HNN_FASTR}\")\n",
    "    print(f\"📌 HNN + FASTR - Left Redundancy Loss: {left_redundancy_HNN_FASTR}\")\n",
    "    print(f\"📌 HNN + FASTR - Right Redundancy Loss: {right_redundancy_HNN_FASTR}\")\n",
    "\n",
    "    print(f\"\\n⚡ **Inference Speed Comparison** ⚡\")\n",
    "    print(f\"🚀 HNN Avg Time: {hnn_time:.4f} seconds\")\n",
    "    print(f\"🛠️ FASTR Avg Time: {fastr_time:.4f} seconds\")\n",
    "    print(f\"🔥 HNN + FASTR Post-processing Avg Additional Time: {hnn_fastr_time:.4f} seconds\")  \n",
    "\n",
    "    plt.plot(model.loss_history)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.title(\"HGNN Training Loss\")\n",
    "    plt.show()\n",
    "\n",
    "def train_and_evaluate(num_attributes=10,\n",
    "                       num_fds=10,\n",
    "                       max_edge_size=5,\n",
    "                       max_rhs_size=3,\n",
    "                       train_size=100,\n",
    "                       val_size=20,\n",
    "                       test_size=20,\n",
    "                       epochs=50,\n",
    "                       hidden_dim=16,\n",
    "                       lr=1e-2):\n",
    "    \"\"\"\n",
    "    Main pipeline: generate synthetic FD sets, create bipartite data,\n",
    "    train an HNN to classify which FDs are in the minimal cover,\n",
    "    then evaluate predictions by reconstructing FD sets and checking closure.\n",
    "    \"\"\"\n",
    "    # Generate random FDs for dataset\n",
    "    train_hypergraphs = [\n",
    "        generate_acyclic_hypergraph(\n",
    "            num_attributes, num_fds, max_edge_size, max_rhs_size\n",
    "        )\n",
    "        for _ in range(train_size)\n",
    "    ]\n",
    "    val_hypergraphs = [\n",
    "        generate_acyclic_hypergraph(\n",
    "            num_attributes, num_fds, max_edge_size, max_rhs_size\n",
    "        )\n",
    "        for _ in range(val_size)\n",
    "    ]\n",
    "    test_hyerpgraphs = [\n",
    "        generate_acyclic_hypergraph(\n",
    "            num_attributes, num_fds, max_edge_size, max_rhs_size\n",
    "        )\n",
    "        for _ in range(test_size)\n",
    "    ]\n",
    "\n",
    "    # Build (attributes, labeled_fds) for each hypergraph\n",
    "    minimal_train_FDs = [minimal_cover(graph) for graph in train_hypergraphs]\n",
    "    minimal_val_FDs = [minimal_cover(graph) for graph in val_hypergraphs]\n",
    "    minimal_test_FDs = [minimal_cover(graph) for graph in test_hyerpgraphs]\n",
    "\n",
    "    train_edge_indices = [encode_hypergraph_torch(graph, num_attributes) for graph in train_hypergraphs]\n",
    "\n",
    "    # Initialize model + optimizer\n",
    "    model = HNN(in_channels=num_attributes, hidden_channels=hidden_dim, out_channels=1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Train model\n",
    "    train_hnn(model, train_hypergraphs, optimizer, criterion, num_attributes=num_attributes, epochs=epochs)\n",
    "\n",
    "    # Evaluate model\n",
    "    evaluate_HNN(model, val_hypergraphs, minimal_val_FDs, num_attributes)\n",
    "    \n",
    "################################################\n",
    "# Pipeline Execution\n",
    "################################################\n",
    "\n",
    "train_and_evaluate(\n",
    "    num_attributes=50,\n",
    "    num_fds=100,\n",
    "    max_edge_size=10,\n",
    "    max_rhs_size=10,\n",
    "    train_size=200,\n",
    "    val_size=10,\n",
    "    test_size=50,\n",
    "    epochs=100,\n",
    "    hidden_dim=64,\n",
    "    lr=1e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
